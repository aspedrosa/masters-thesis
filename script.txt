 vim: fdm=marker

{{{

Boa tarde, vou apresentar a minha dissertação de mestrado com o título "Desenvolvimento de uma arquitectura escalável para extrair metadados de bases de dados médicas distribuídas", que foi feita com a supervisão do professor José Luís Oliveira.

}}}
{{{ context 1

É muito comum investigadores quererem fazer investigação sobre registos eletronicos de pacientes, os chamados EHR, para, por exemplo, saberem qual o impacto de um determinado farmaco em diferentes faixas etarias, que tipos de medicamentos são percritos em determinadas situações, etc.

No entando devido ao elevado número de registos numa base de dados EHR, é necessario primeiro saber se a bases de dados tem pacientes que pertencem à determinada populção que se pertende ser estudada, como por exemplo, saber se tem registos de pacientes com COVID, ou pacientes com Diabetes, se tem prescições, etc.

No entanto, esta analise muitas vezes não pode ser feito pelos investigadores porque os donos das bases de dados não querem, ou mesmo legalmente não podem dar acesso direto à base de dados.

}}}
{{{ context 2

Portanto, com isto como pode ser transmitida informação confidencial das bases de dados para os investigadores?

Uma solução é Database profiling, em que o objetivo é transmitir o máximo de informação transmitindo o minimo de informação.
Com este metodo, associado a cada base de dados, são expostos metadados, que é informação resumida da base de dados sem ser com uma granilidade muito fina.
Os investigadores podem depois usar estes metadados para saber se uma determinada base de dados contem registos que podem ser uteis para a sua investigação.

}}}
{{{ context 3

No departamento existe já um historial de soluçoes que permitem isto, nomeadamente a plataforma chamada MONTRA, que já servio como base para desenvolver catalogos de bases de dados para projetos a nivel europeu.

Até agora, já está resolvido o problema de dar informação sobre as bases de dados para que os investigadores possam saber se a base de dados tem informção relevante para responder a uma determinada pergunta cientifica.
Agora, como é que estes metadados são atualizados?
Atualemente isto é feito manualmente, o que lava a que este metadados facilmente fiquem desatualizados.

}}}
{{{ objectives

Com isto, o objetivo desta dissertação é por um lado melhorar e otimizar a framework MONTRA de forma a apresentar e a gerir os metadados de forma correta e por outro lado automatizar o processo de atualização dos metadados presentes no MONTRA.

Para isso é necessário existir uma ferramenta capaz de extrair os metadata de bases de dados clinicas e tembém ter um sistema que faça chegar estes metadados extraidos a aplicações, como o MONTRA, para que se mantanham atualizadas.

}}}
{{{ background

Para além do MONTRA fizemos um levantamento do que soluções semelhantes têm.
Fizemos uma avaliação comparativa com vários fatores como:
- se seguem uma filosofia de codigo aberto, permitindo possiveis alterações;
- se contêm mecanismos de proteção de dados já que estas lidam com dados pessoais;
- e se permitem a interação facil com os dados tanto para os humanos como para as maquinas;

O objetivo era perceber se podiamos tirar partido de funcionalidades presente em outras soluções ou eventualmente até mesmo mudar para uma plataforma diferente se chegassemos à conclusão que tinhamos ferramentas open source melhores da que a usada atualmente.

Foram também consideradas outras ferramentas tendo em conta as suas funcionalidades de extração de metadados e também da sua possiblidadde de criação de redes metadados de bases de dados.

No fim não encontramos uma plataform que atingise todos os requisitos portanto teriamos de ou desenvolver uma solução ou fazer a intergração de soluções existentes.

}}}
{{{ montra

MONTRA was proposed as a flexible base architecture for building biomedical data integration platforms, allowing to centralize and share data from heterogeneous sources.
This last point is achievable, by requiring the definition of a metadata skeleton, which describes the original data.
Whi this, different data sources following the same skeleton will have a common representation.
Then, the framework allows users to view, search, modify and delete information, where access is controlled via a Role-Based Access Control system.

}}}
{{{ metadata visualization

Agora passamos à descrição da plataforma usada capaz de armazenar e expor metadados.
Decidimos continuar com o MONTRA, principalmente devido ao seu design centrado em volta de bases de dados.
Para alem disso, como permite expor os dados atraves de um API, tanto humanos como máquinas podem facilmente interagir com a plataforma, por outras palavras, segue as directrizes FAIR, que mencionamos no background e que procuravamos numa plataforma deste tipo.
O MONTRA foi desenvolvida em Django, uma web framework de alto nivel contruida em Python, que permite o facil desenvolvimento de aplicações web criando abstrações em cima de conceitos mais dificis de manter e/ou desenhar.

Antes de entrar em mais detalhes sobre o que foi feito nesta ferramenta, primeiro é importante falar em alguns conceitos chave da plataforma:
- esta pode ser dividida em várias comunidades, sendo uma forma de criar redes ou groupos dde bases de dados no mesmo portal.
- Depois, associado a cada comunidade, ao esqueleto que descreve o conteúdo original de uma base de dados, chama-se questionário, que permite definir uma série de perguntas que os donos das bases de dados devem responder para descrever os dados da sua base de dados.
- Uma fingerprint é o nome dado ao conjunto de respostas a um questionário.
  Por outras palavras, são os metadados que descrevem a fonte original dos dados.

}}}
{{{ fingerprint views

O MONTRA apresenta um conjunto de interfaces semelhantes para manipular os dados de uma fingerprint.
As interfaces são as seguintes:
- de criação
- edição
- procura
- e de consulta

Estas diferentes interfaces foram desenvolvidos de uma forma que não tirampartido do sistema de formulários integrado do Django, com isso, toda a validação de dados é feita do lado do cliente através de código javascript, sendo esta facilmente contornada fazendo chamadas diretamente à API do MONTRA.

}}}
{{{ fingerprint views retangulos

Além disso, para além das diferentes variantes da interface de manipulação da fingerprint terem um aspecto semelhante, todas utilizam um modelo diferente, assim, se for feita uma alteração a um componente comum, essa alteração tem que ser aplicada ao mesmo componente nas outras variantes.

}}}
{{{ fingerprint schema

Os questionários são definidos através de um ficheiro Excel, onde o gestor gestor de uma comunidade deve detalhar cada pergunta e secções do questionário.
Aqui podemos ver um exemplo simples de um grupo de perguntas que aceitam texto aberto.

}}}
{{{ fingerprint schema multiple choice

Mas a estrutura deste excel sofre de clutter, sendo dificil lerinformação presente em algumas celulas.
Por exemplo, para definir uma questão de escolha múltipla, todas as escolhas são definidas numa coluna de "Value list" separada por uma barra vertical.
Depois, se uma escolha específica suporta um texto adicional, temos de acrescentar esta três pontos.
Como é evidente, de imediato é difícil de ver quais são as escolhas e qual será o resultado final a partir desta configuração específica.

}}}
{{{ fingerprint schema choice tabular

Mas isto torna-se pior para tipos de pergunta mais complexos, como este que permite ter uma tabela, onde o utilizador tem de definir as colunas, linhas e o tipo de input fornecido pelo utilizador.
Assim, aqui na coluna "Value list" temos colunas separadas por barras laterais, depois este separador de dois backslashes, depois linhas, depois novamente este seprador e o tipo de input.

}}}
{{{ data models

Relativamente aos modelos de dados que armazenaram tanto a estrutura do questionário como os dados das respostas das fingerprints.
Todos os dados das respostas são armazenados em texto, portanto, se houver uma pergunta do tipo numérica no questionário, em vez de ser armazenada como um número é armazenada em texto, ou seja o espaço da base de dados não é otimizado.

A estrutura do questionário, é armazenada utilizando apenas 4 modelos de dados, o que leva a que o modelo que guarda informação sobre perguntas tenha um elevado número de campos, principalmente para especificar as configurações para tipos de perguntas mais complexas.

}}}
{{{ data models 2

Em outros modelos podemos ver duplicação de dados, tais como os dados de respostas são armazenados tanto num modelo de resposta como num modelo de mudança de resposta, uma vez que MONTRA contém o histório de respostas para uma dada fingerprint.

Em vários modelos podem também ser encontrados alguns tipos incorrectos a serem utilizados.
Tal como no modelo de dados do questionário, o campo "disable" utiliza um campo tipo caractere em vez de um campo booleano.

}}}
{{{ Refactoring models

Com todas estas falhas detectadas, decidimos realizar um processo de refactoring na plataforma, com o objectivo de não alterar a forma como os casos de uso são executados, mas sim melhorar o desempenho e a capacidade de manutenção do sistema.

Relativamente aos modelos de dados, criámos novos modelos para armazenar dados de respostas específicas do tipo.
Portanto temos o modelo principal, Answer, e depois os próprios dados são armazenados em resposta de texto ou resposta de datas e assim por diante.

Houve algumas outras correcções, sendo uma delas a remoção de vários campos do modelo de dados Question que definiam configurações extras relacionadas com especificos tipos de perguntas e criamos mais modelos para armazenar essas configurações extra, tais como o Label, a Choice-tabular Question e entre outros.

}}}
{{{ Refactoring views

As interfaces de manipulação das fingerprints, marcados com retangulos amarelos, foram divididas em componentes reutilizáveis, marcados a azul, que foram depois utilizados para construir um único modelo da interface de uma fingerprint.
Com isto, uma alteração feita a um componente partilhado, terá efeito nas várias variações da interface da fingerprint.
Isto foi concretizado devido à funcionalidade de renderização condicional do Django, podendo um determinado componente ser renderizado apenas numa determinada variante.

}}}
{{{ Refactoring data manipulation

Quanto à api que enviava os dados das respostas para o back end, foi refeita, fazendo uso do sistema de formulários integrado do Django, com isso, toda a validação de dados é agora efectuada no back end.

}}}
{{{ refactoring excel

Finalmente, a estrutura do excel que é utilizada para definir um questionário foi melhorada para resolver o problema de clutter.
Aqui está um exemplo de como um tipo de pergunta era, e como é agora definido.
Principalmente os dados estão agora espalhados por mais linhas, e para isso criámos mais tipos de linhas, em vez de termos apenas 3 tipos básicos, evitando assim ter celulas com demasiada informação.

}}}
{{{ Metadata extraction update 1

Agora passamos à parte do sistema que extrai metadados das bases de dados e envia-os para aplicações, para que estas se mantenham actualizadas.
Por aplicação estamos apenas a considerar uma aplicação web que espera dados através de pedidos HTTP.

Aqui temos uma arhitectura de alto nível do sistema desejado.
Agentes são um software que corre no ambiente de produção dos donos dos dados, com o papel de extrair metadados e enviá-los para as aplicações.

Como os proprietários dos dados podem não querer ou legalmente não poder fornecer acesso directo aos dados, decidimos dividir a extracção e envio de metadados em dois componentes.

}}}
{{{ extraction & update 2

Os agentes agora só enviam dados para as aplicações e o processo de extracção é tratado pelos dono dos dados.

No entanto, ainda temos de pensar em como os dados chegarão às aplicações.
Uma possível abordagem seria criar uma rede peer-to-peer de agentes, e estes armazenariam a informação sobre aplicações para depois lhes enviarem dados.

}}}
{{{ extraction & update 3

Contudo, os donos dos dados podem não querer gastar os seus recursos na manutenção de uma rede peer-to-peer.
Para isso, decidimos que uma melhor abordagem é ter uma componente central que receba os dados dos agentes e os envie para as aplicações.

}}}
{{{ extraction

# NO LER a variante do EHDEN do achilles apresenta ainda mais sumarios, arredondando os valores para multiplos de 100 por exemplo, para anonimizar ainda mais os dados


For the extraction process, we decided to use a software called ACHILLES, which can produce summaries and metadata of databases conforming to the OMOP CDM.
These allow for characterization and quality assessment of observational health databases.
It is implemented as a package written in R, which executes a series of SQL queries over the original CDM database to calculate all the specific summaries.
The resulting summaries can be used by researchers to evaluate if the contents of databases can be used on studies that they intend to perform.
As this part of the sytem has direct acess to data we left the setup of it to the data owners.

To have communication between the central component and the agents and to avoid requesting the data owner to have open ports on their production system, we made use of an asynchronous message system, named Kafka.
With this agents will pull information from kafka instead of data being pushed by an open port.

Due to Kafka's entire ecosystem and its capabilities for data stream processing we also used it to transport metadta from the agents to the central componet.

With data we had to insert extracted metadta into kafka.
This was done using Kafka connect, which allows to easily stream data between Kafka and external systems.

The agent is distributed as a docker image, to simplify deployment by the data owner, which contains two internal components.
A health check handler that is used by the central component to check if a certain agent is active.
And then a kafka connector that loads metadata stored in a file, provided by the extraction tool.

}}}
{{{ metadata update

With data in kafka, there are some other connectors that can send kafka messages through http requests.
However, these do not offer any type of customization to build the http request, which is important since different application expect the data in different formats.

Also as data in kafka is treated as an unbounded data flow, and data is uploaded into kafka in several messages by the agent, there needs to be some system that works on top of kafka to fix these problems.

This system is composes by 5 componetns.

}}}
{{{ filter worker

As the destionation application might not require all the metadta that is extract, is is important to first first the data.

For this we have this component that can have several filters running over the data uploaded from an agent.

A single filter worker components can handle data from one agent at the time.
To handle multiple, several instances of this components can be deployed.

}}}
{{{ orchestrator

With this, a orchestrator component exists to distrube the data from the agent acrros the exsiting filter worker instances.

}}}
{{{ sender

once data is filtered, the sender component is in charge of sending the data to the applications.
It allows the system admin to customize the http request by requiring a template with placeholders, where the sender will then fill with data once it recives data from the filter worker component.

}}}
{{{ admin protal

The admin portal component provides a web interface to the system admin to interact with the system, mainly to register new databases and applications and also to get feedback on data that is flowing on the system.

}}}
{{{ stastics recorder

Finally the statistics recorder component, captures messages flowing through the system and stores them in a relational database so they can be consulted on the admin area.

}}}
{{{ Demo

I will now show demo of the system working.
Here we will have a community, which has a database, lets consider the hospital of aveiro.
two filters: one that extracts only the patient count and another that does no filter on the data recived.
then we have two web applications:
one is an installation of the MONTRA framework,
and another is the network dashboards tool.

}}}
{{{ web

This last, is a tool that I helps to build from the ground up which is integrated with the EHDEN portal which was developed whit the MONTRA framework.
It allows to upload csv files generated by the specific variation of the ACHILLES Tool developed by the EHDEN project.
THen it uses this data to generate charts to allow analyze and comprare CDM databases.

Here I have an intalation of montra with a simple quetsionnaire
and here is the network dahboards tool page to upload the file

here I have a sample file with the same structure of the one generated by the extraction tool.
I will uploaded into the agent, which is done by simply moving it into a directory where the agent is watching for new files.

As we can see both the data on the MONTRA installation and on the network dashboards was uploaded

}}}
{{{ results

As results with built  a system capable of achieving the objectives of metadata extraction, mediation, and visualization.

Several improvements were done to a fully-fedge metadta visualization, which are in a pull request waiting for review.

It was proposed a tool able to extract metadata from a database conforming to the OMOP CDM, taking into account restrict data privacy concerns of data owners.

Finally it was developed a system capable of both gathering and sending metadata to applications, maintaining them with data up-to-date.

}}}
{{{ conclusion



}}}
