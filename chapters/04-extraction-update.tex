\chapter{Automatic Metadata Extraction and Update}
\graphicspath{{figs/04-extraction-update/}}

As we have an application ready to receive and display metadata, it is now crucial to turn the attention to extracting metadata from \gls{cdm} databases, which contain the actual data, allowing, then, to update the data stored on applications such as the MONTRA framework.

Figure \ref{fig:overall-arch-v1} presents a high-level architecture of the desired system that extracts data from databases and sends it to the applications that need their local data updated.
Agents are software that runs on the data owner's deployment, in charge of extracting metadata from the database and sending this metadata to the applications.

\begin{figure}[H]
    \center
    \includegraphics[width=.6\textwidth]{overall-arch-v1}
    \caption{High-level architecture of the extraction and update process.}
    \label{fig:overall-arch-v1}
\end{figure}

\section{Requirement Analysis}

When we enter the realm of accessing an institution's sensitive data, some problems start to arise.
Not all data owners are willing to, or legally can't, provide direct access to their data.
When building this part of the system, we have to consider this and have several options available that use different levels of access and according to the demands of each specific data owner, use the most appropriate one.
In one end, a single solution could have full access to the data, extracting the metadata directly from the original data.
At the other end of the scale, the burden of extracting data is removed from us and the system is dependent on the data owner providing the metadata and only then the system processes the metadata.
Having several options for different access levels is more flexible, however, it requires maintaining several tools which are not reliable.
A more appropriate alternative is to have a single option, that is designed considering the most strict access to the data, however, we still need to provide a tool to extract the metadata.
The only difference is that we are not the ones executing it.

As the extracting tool has direct access to the data, using an existing and widely known tool is a must, so data owners are inclined to use it.
Additionally, making fewer to no changes is preferable to ensure that data owners don't discard the tool because they don't trust the changes.

\begin{figure}[H]
    \center
    \includegraphics[width=.6\textwidth]{overall-arch-v2}
    \caption{High-level architecture of the extraction and update process}
    \label{fig:overall-arch-v2}
\end{figure}

In figure \ref{fig:overall-arch-v2} is presented the overall architecture of the system, where the agent is the tool in charge of parsing the metadata provided by the data owner, which is then sent to the applications.
Regarding this last point, there needs to be a way for the data to go from the agents to the applications.
One ambitious solution is to create a decentralized peer-to-peer network of agents, avoiding developing a central component.
In such solution, the agents are in charge of managing and sending the data to the applications.
However, data owners might not want to spend their hardware resources to maintain a network.
Since the agents are executed on the data owner's system, they should do only the required and minimal functionality, spending the few resources as possible.

A more executable solution is to have an intermediate component that receives the metadata from the agents and then distributes that data across the applications, following a centralized network architecture as it is presented in figure \ref{fig:overall-arch-v2}.

\begin{figure}[H]
    \center
    \includegraphics[width=.6\textwidth]{overall-arch-v3}
    \caption{High-level architecture of the extraction and update process, with a centralized network approach.}
    \label{fig:overall-arch-v3}
\end{figure}

This central component will be used to manage the data and the applications to where such has to be sent.
However, it is also crucial that feedback is provided on the system's state, such as how the data is flowing, which applications received which data, and which databases sent data into the system.

% aqui digo que percisamos de um componente intermedio. mais á frente vou dizer que percisamos um componente intermedio entre o kafka e as aplicações. coisas diferentes

On the applications side, there are other important factors to consider before going forward on implementing the intermediary component mentioned before.
First, then we talk of application we are considering only web application that expect to receive metadata through a \gls{rest} \gls{api}.
Second, the \gls{api}s of different applications expect the metadata in distinct arrangements.
A way out of this would be to specify a set of endpoints that each application had to implement, yet that would lead the applications to have two sets of endpoints that do the same thing.
On the other side, making a flexible option, where the requests where the data is sent are customizable, would be more appealing for the target applications, which would not have to make any changes.
Third, not all applications require all metadata data that is extracted from a database.
Some use all the data, others only need a value of such data.

\subsection{Functional Requirements}

\begin{enumerate}
    \item Databases must be registered on the central component
        \begin{list}{}{}
        \item By having a record of the databases that are attached to the network, the system can avoid accepting data from databases that were not registered in the system.
        \end{list}

    \item It must be possible to check what agents are active at a certain point
        \begin{list}{}{}
            \item This allows giving feedback on the state of the network to the admin.
        \end{list}

    \item The system should allow managing the destination applications
        \begin{list}{}{}
            \item The applications to where the data of the databases are sent, should not be hard-coded, for that, the network admin should be able to perform \gls{crud} operations on such.
                The term application should be interpreted as web applications that have \gls{rest} \gls{api} endpoints available to receive data.
        \end{list}

    \item The system should provide statistics on the data that has circulated
        \begin{list}{}{}
            \item This is yet another feature to give feedback to the network admin about, now allowing to both check the amount of data that each database sent, and the amount of data that each application received.
        \end{list}

    \item The format of the request sent with the metadata to each application must be customizable
        \begin{list}{}{}
            \item This is important since different applications expect to receive metadata in distinct formats since each application has a different \gls{api}.
        \end{list}

    % eu não falo aqui das pipelines, que permitem filtrar os dados, pk isso foi algo que adicionei como consequencia consequencia em termos usado kafka

    \item The system should allow to group Databases
        \begin{list}{}{}
            \item In a scenario where both all the destination applications and all databases belong to the same project, the metadata of the databases makes sense in all applications.
            \item Now consider a scenario where half of the databases are associated with one project and the rest with another.
          Each project has a destination application that receives the data.
          Considering that there is no concept of project or groups, both applications will receive data of both projects.
          To solve this, it required to have two installations of this system, one for each project.
          By grouping databases, data of a database can be guided to applications that are also attached to the same group.
        \end{list}

\end{enumerate}

\subsection{Non-Functional Requirements}

\begin{enumerate}
    \item The data owner can stop the agent at any time
        \begin{list}{}{}
            \item This gives yet another layer of control to the data owners over their data.
          If at any time, they don't want to share their data, they can stop the agent.
          This also poses a constraint in the system that it should not be designed assuming that the agents, once deployed, will always stay active.
        \end{list}

    \item The extraction tool should be based as much as possible on an already existing solution
        \begin{list}{}{}
            \item Using a working solution avoids having to build a new tool from scratch.
              But apart from that, using a known solution as a base helps to gain the confidence of the data owners so that they use the resulting solution in their databases.
        \end{list}

    \item The agents must not have direct access to the data
        \begin{list}{}{}
            \item This option assumes the least amount of access to the user's data so data owners are receptive to allow agents to be run on their local deployments.
              This will require that an agent has access to at least a shared storage where agents check for new data and data owners post their extracted metadata.
        \end{list}

    \item The agent should easily deployable
        \begin{list}{}{}
            \item As the installation of the agent will be done by a person not familiar with the project, this process should be as smooth as possible and well documented.
        \end{list}

    \item Agents should spend few resources
        \begin{list}{}{}
            \item Considering agents run on data owners' deployment environment, it should have a low resource footprint so it does not impose any disruption.
              For that, they must be designed to perform only the required task.
        \end{list}

    \item The system should not require any open ports on the data owners' deployment environment
        \begin{list}{}{}
            \item As there might be communication between the central component and the agents, requesting an open port on the data owners' deployment environment might not be possible since it could require them to change firewall settings as was mentioned in \cite{popmednet}.
        \end{list}

    \item Scalable
        \begin{list}{}{}
            \item As more databases are connected to the network, the system should allow increasing resources so that it handles processes faster.
        \end{list}

    \item Microservice architecture
        \begin{list}{}{}
            \item The system should be composed of simple and replaceable components that deal with well-defined tasks.
        \end{list}
\end{enumerate}

\subsection{Use Cases}

From the previous requirements defined, there can be outlined two actors that will interact with the system:

\begin{itemize}
    \item Data owners, which maintain the agent on their local deployment;
    \item System admin, that interacts with the central component to manage the whole system.
\end{itemize}

In figure \ref{fig:use-cases} is presented the use-case diagram, which shows the actions that each actor performs on the system.
Such actions were divided into three groups: Agents, Metadata Extraction, actions on the central component related to receiving data from the agents, and Metadata Update, actions on the central component related to sending data to the applications.

\begin{figure}[H]
    \center
    \includegraphics[width=\textwidth]{use-cases}
    \caption{Use case diagram of the metadata extraction and update system}
    \label{fig:use-cases}
\end{figure}

\begin{enumerate}
    \item Set up the agent on the local deployment system
        \begin{list}{}{}
            \item Since it's not expected that data owners provide access to their deployment environment, the responsibility to set up the agent is imposed on them.
        \end{list}

    \item Stop the agent
        \begin{list}{}{}
            \item As Data owners control their data and the access to it at all times, them stopping the agent should be an action expected by the system.
        \end{list}

    \item Register a database
        \begin{list}{}{}
            \item As new databases enter the network and agents are deployed to gather their metadata, the system admin needs to register new databases on the central unit, so the data received is accepted into the system.
        \end{list}

    \item Create communities
        \begin{list}{}{}
            \item In a scenario where there are several databases from distinct projects, there is the need to group databases to treat their data separately.
        \end{list}

    \item Associate a database to a community
        \begin{list}{}{}
            \item Once there are databases and communities registered in the system, the system admin can do many-to-many associations between them.
        \end{list}

    \item Check active agents/databases
        \begin{list}{}{}
            \item At any time, the system admin can check the state of the network and verify which databases have an agent active.
        \end{list}

    \item Analyze statistics of data received from an agent/database
        \begin{list}{}{}
            \item As agents send data to the central component of the network, statistics can allow the system admin to see what databases contribute more data to the network and its frequency.
        \end{list}

    \item Register an application and customize how data is sent
        \begin{list}{}{}
            \item The system admin will also define the destination applications for the data received from the agents, setting also the format of the request that sends the data to such applications.
        \end{list}

    \item Analyze statistics of data sent to an application
        \begin{list}{}{}
            \item As metadata enters the system, having statistics of data sent to the application helps check if the system is working properly and if the applications are receiving the expected data.
        \end{list}

\end{enumerate}

\section{Extraction}

First lets go over the portion of the system which runs in the part of the data owners

\subsection{ACHILLES}
\begin{itemize}
    \item percisade de um vocab files
    \item escolhemos principalmente por estar direcionada para bases de dados cdm
    \item organização interna
    \item implementado em R
    \item diferentes maneiras de exportação (json, csv ou diramente para db)
    \item a query for each analaysis
    \item Catalogue Export
\end{itemize}

\subsection{Asynchronous Message Systems}

isto é preciso para haver comunicação enter agents e a central unit

\subsubsection{RabbitMQ}

\subsubsection{Kafka}


\subsection{Kafka Source Connectors}
\begin{itemize}
    \item fetch from files or from tables
    \item falar nas varias opcoes consideradas. as the tables não davam tudo duma vez, e as alternativas de ficheiros não davam feedback de quando tudo ja tinah sido uploaded
    \item o que eu escolhi da feedback de quando todos os records estão kafka e quantos records foram escritos
\end{itemize}

% ---

\subsection{agent final architecture}
\begin{enumerate}
    \item data owners should automatize (if they want) the process of achilles extracting data from their databases
    \item send data to kafka
    \item responde to health check messages
\end{enumerate}

\begin{figure}
    \center
    \includegraphics[width=\textwidth]{agent-architecture}
    \caption{agent arch}
    \label{fig:agent-architecture}
\end{figure}

\section{Publishing}

data is in kafka. what do we have to do to reach the applications

\begin{itemize}
    \item need to send custom data on a CUSTOM FORMAT to a custom REST endpoint
    \item Kafka Sink Connectors (The target REST API is not customizable, such as the data format ) - need for a sender application
    \item No known end on kafka topics/streams - require some management on top of kafka
\end{itemize}

\section{Network Manager}  % change name
\begin{itemize}
    \item centralized entity
    \item transforms the data to the required format
    \item sends to the specific application's endpoint
    \item 5 components
\end{itemize}

\subsection{Pipelines Workers}
\begin{itemize}
    \item lets do it in go cause lets explore
    \item search for kafka library
    \item ksql introduced. easily filter data with select \_ where \_
    \item select and filter the data comming from the databses
    \item only process data of a single database at a time. handling multiple is the same is hard to handle and hard to scale
    \item scalable: several pipeline worker applications
\end{itemize}

\subsection{Orchestrator}
\begin{itemize}
    \item To ensure multiple records of multiple databases are not processes at the same time, this component redirects the data from databases to the pipelines workers preventing the previous problem
    \item why java? (only it contains a kstream api) and we use it to redirect messages from one topic to another
\end{itemize}

\subsection{Sender}
\begin{itemize}
    \item sends/publishes the data resulting from the pipelines, to the application's endpoints
    \item why python? cause jinja and pandas (people know how to use)
\end{itemize}

\subsection{Admin Dashboard}
% nice implementation detail: only on the backend of this are topics created

\begin{itemize}
    \item search for tecnologies (first golang pls . https://github.com/qor/admin)
    \item 2 components: api backend (django) + react frontend
    \item manage the whole thing
    \item Allows to perform all the use cases
\end{itemize}

\subsection{Statistics Recorder}
\begin{itemize}
    \item Listens to the kafka topics that are used along the system and transforms into persistent data (sql)
\end{itemize}

\section{Network Data Flow}
\begin{itemize}
    \item Evaluation?
    \item step by step
    \item number of topics involved
\end{itemize}
