\chapter{Background}
\label{chapter:background}

%Several biomedical researchers perform studies to know the effect of certain drugs
From a medical standpoint, to perform their studies, biomedical researchers need to
contact data owners to have access to relevant data that can help improve their
analysis and/or findings that can be applied to real cases.

% estudam doenças
% percisam de fazer analyses
% isto percisa de dados

With this procedure emerges several problems for the researcher such as he has to find
institutions willing to share data and the process of contacting the data providers can
be cumbersome.
To aid in this whole process, several data hubs have been developed with the purpose of
making the process of data discovery easier.
One important aspect of such data hubs is that they present to the researcher meta
data, which is aggregations or summaries of the original data.
Metadata has the advantage that one doesn't have to deal with the anonymization process
of medical records, since only summaries of the initial data are retrieved
~\cite{egenvar, montra}.
With this dependency on the original data, emerges an important problem of data hubs
which is, metadata can easily be outdated after a small time window.
This could not raise a big problem, if the records were updated regularly, however this
rarely happens, mainly because either the update process is difficult or because
metadata has to be manually extracted and uploaded to the data hub.
A problem that still might arise from such platforms, is those different datasets very
often have different representation for the same concept or the data is organized in a
different layout.
The research is then hampered since either different approaches have to be taken to
analyze each dataset.

The \gls{ehden} project has affiliations with several institutions, data sources and
data custodians across the \gls{eu}, which the main goal is to, within a federated
network, harmonize their data to the \gls{omop} \gls{cdm}~\cite{ehden-datapartners}.
With a \gls{cdm}, the problem of having different representations for the same data
across distinct data sources is solved.
Researchers can now develop a single analysis method and then apply it to all gathered
datasets and these methods can be optimized for this specific data model, which allows
large-scale analytics.
Furthermore, also improves collaborative research~\cite{ohdsi-site}.
Still, within the scope of the \gls{ehden} project, the project has a database catalog,
built with the Montra framework~\cite{montra}, where data owners fill metadata about
their data source manually, which brings the outdated problem already mentioned before.
Additionally, whenever new metadata fields are introduced, the data owners of all data
sources have to go manually update their metadata form.

To build a valuable data hub is then important to take into account how to:
\begin{itemize}
    \item extract metadata from a data source
    \item upload and update the metadata on the data hub
    \item automatize the two processes mentioned before
    \item receive and display the metadata on the data hub in a way that facilitates
        readability.
\end{itemize}

\section{Metadata Visualization Tools} \label{sec:viz-tools}

%30876434,Evaluation of repositories for sharing individual-participant data from clinical studies.
%31862012,The Systematic Review Data Repository (SRDR): descriptive characteristics of publicly available data and opportunities for research.

It will then be explored existing visualization platforms that enhance data discovery
by presenting summaries or metadata of records (data sources, datasets).

In some cases data can't be publicly available because it contains sensitive data or
simply the data owner might not want to share some portions of the data, for that the
tools analyzed should have privacy protection mechanisms, allowing to customize the
access and manipulation of data stored.

Furthermore, considering we want to improve and assist data discovery and reuse it is
important to have good data management to simplify such processes.
However, humans fail to achieve the necessary processing levels with present-day
scientific data.
It is then important that data is provided in such a way that machines can fetch,
understand, analyze and act on data.
For that the \gls{fair} Guiding Principles were established which contain a series of
considerations for data publishing to supports both human and machine operations such
as deposition, exploration, sharing and reuse~\cite{fair}.

%32620019,From Raw Data to FAIR Data: The FAIRification Workflow for Health Research.

Finally, it is preferential for such a tool to be open source since the available
solution might need some changes to solve our specific problem, and also it makes it
possible to receive contributions from the community.

\subsection{Search Method}

Regarding this subject, there was already done a systematic review of several tools
that fit within the current search pool.
Its objective was to ``identify projects and software solutions that promote patient
electronic health data discovery, as enablers for data reuse and advancement of
biomedical and translational research''~\cite{systematic_review}.
From the final 20 systems, they captured their interoperability, what type of data they
were providing and their after effect related to scientific results and improvements to
better healthcare.
To perform their search they only used PubMed
\link{https://pubmed.ncbi.nlm.nih.gov/} considering it indexes a substantially
amount of health-care related work and provides a public \gls{api} which allows
automation of the retrieval process.
The programmatic retrieval was done using the Biopython
framework\link{https://biopython.org/} where all search queries were limited to the
time windows between January 2014 to September 2018.

A softer version of the previous systematic review was done now within November 2018
and January 2021.
Also, instead of doing a query to PubMed and then find related publications, the
process taken here was to skip the first step and find related publications of the
final 16 selected on the systematic review and then try to find some software solutions
of interest.

\subsection*{eGenVar}

A software suite called the eGenVar\cite{egenvar} data-management system, allows users to report,
track, and share metadata on data content, origin and history of files, without
compromising privacy or security, since could be used to search data while the original
files remain in a secure location.
Users need to have an account to access the system and once created can immediately
start using the system for search operations.
However, a personal profile needs to be created before adding, deleting or updating
content.
It is designed to connect current Laboratory Information Management Systems and
workflow processing systems and to keep origin information for data being processed
through distinct systems at different locations.
Central to the system is a tagging process that allows users to tag data with new or
pre-existing information, such as ontology terms or controlled vocabularies, at their
convenience.
The system includes a server, a command-line client, other clients that can be
developed in several programming languages and a web portal interface.


\subsection*{MONTRA}
A base architecture for designing data integration platforms, with emphasis on
biomedical data.
It has a flexible architecture for centralizing and sharing such data coming from
multiple, heterogeneous sources.
The data sources that are cataloged within the system contain the full data, while the
views available through MONTRA\cite{montra} provide a skeleton of these underlying sources.
The skeleton definition is a straightforward operation that can be done by any data
custodian and can be saved as a spreadsheet file and submitted through the web
interface.
Furthermore, the user interface allows viewing, searching, modifying and deleting
information through simple forms.
Also, a RESTful API is available which provides a set of programmatic endpoints that
can be consulted by third-party applications.
Access is controlled via a Role-Based Access Control system to guarantee that proper
constraints are imposed.
Different data sources following the same skeleton template will have a common
representation within the platform.
By using a common structure as a metadata skeleton, distinct data sources converge to a
homogeneous representation of their metadata.

This framework is in used to deploy the \gls{emif} Catalogue, a platform that aims to
be a marketplace where data custodians can publish and share information about their
clinical databases, while biomedical researchers can search for databases that fulfill
their particular study requirements.
Currently, the \gls{emif} Catalogue supports several distinct projects, combining, for
instance, data available in pan-European Electronic health record and Alzheimer
cohorts.

\subsection*{REDCap}
Recognizing the need of those participating in research to be able to securely collect
and share data, a team at Vanderbilt University developed \gls{redcap}~\cite{redcap}, a data and
metadata gathering tool.
\gls{redcap} data capture tools can either be structured to function as a series of
forms that investigators fill out as they advance through a project or as a survey
designed to be completed by subjects.
With \gls{redcap}'s collaboration functionality, investigators after adding team
members to a project can assign abilities and access to each, based on their roles and
data needs.
\gls{redcap} includes several features to help assure data quality.
The Data Quality reports not only identify missing values and outliers, validation
errors, and incorrect values for calculated fields but also allow users to create their
own custom rules to assess data correctness.
Collected data can be observed within \gls{redcap}, which provides views with basic
statistical measures, but also supports exportation for several common formats.
Users may import previously collected data using the Data Import tool.
Furthermore, offers an API to support remote entry and retrieval of data.
Finally, there are also many features that enable support for various types of clinical and
basic science research.

In \cite{vanderbilt} the Vanderbilt research data warehouse framework is explained,
which consists of identified and de-identified clinical data repositories and tools
built on top of the data layer to assist researchers across the enterprise, being one
of them \gls{redcap}.
Finally the Ontario Brain Institute’s ''Brain-CODE``~\cite{braincode} is a
neuroinformatics platform designed to support the collection, storage, sharing and
analysis of different data types across several brain disorders, as a means to
understand common underlying causes of brain dysfunction and develop new approaches to
treatment.
In here \gls{redcap} is used to collect demographic and clinical data.

\subsection*{Data Sphere}
The \gls{pds}~\cite{datasphere} provides a platform on which industry and academia can share
raw data from late-phase oncology clinical trials.
Providers of data to this project sign a data sharing agreement were contains some
metadata about the data that is being proposed to upload, which then they have to
follow instructions to upload the given data to the platform.
On approval of the data application, authorized users can access and download all
datasets on the platform.
To get be authorized, the project only requires an application with information about
the background of the individual requesting access and an agreement to the terms of
use.
This level of openness avoids having to apply for a single data set at a time and
allows researchers to pool data from several different sponsors to develop a better
cohort for analysis.
Patient privacy is in the responsibility of the data providers, where they have to
deidentify data according to standards set forth by the Privacy Rule of the U.S. Health
Insurance Portability and Accountability Act of 1996.

As of January 2021, the \gls{pds} website had available cancer trial data from over 150
trials including over 100,000 subjects \cite{datasphere-site}.

\subsection*{Molgenis}
MOLGENIS~\cite{molgenis} is a generic, model-driven toolkit for the rapid generation of custom-made,
data-intensive biosoftware applications.
Bioinformaticians can use a domain-specific language, implemented using XML, to
efficiently model the biological details of their particular biological system, which
enables compact specification of what experiment database is needed.
The level of abstraction is raised, so no lengthy, technical or redundant details on
how each feature should be implemented in general programming languages have to be
given.
Bioinformaticinas can also use MOLGENIS software generation tools to automatically
generate a web application tailored to the experiments of their biologists, building on
reusable components.
The authors found up to 30 times efficiency improvement compared to hand-writing
software, while providing a richness of features practically unfeasible to produce by
hand but not yet provided by related projects.

The \gls{bbmri-eric} project~\cite{bbmrieric} provides fair access to
quality-controlled human biological samples and associated biomedical and biomolecular
data, enabling the investigation of basic mechanisms underlying diseases, indispensable
for the development of new biomarkers and drugs.
Here MOLGENIS is used to develop their Directory 1.0 which provides an overview of the
\gls{bbmri-eric} ecosystem with its distributed structure of National Nodes, and even more
importantly to help users identify biobanks of interest to them.
RD-Connect~\cite{rdconnect} is a global infrastructure project initiated that links
genomic data with patient registries, biobanks, and clinical bioinformatics tools to
create a central research resource for \gls{rd}s.
Its composed by the RD-Connect Registry \& Biobank Finder, a tool that helps \gls{rd}
researchers to find \gls{rd} biobanks and registries and provide information on the
availability and accessibility of content in each database.
The finder is also a portal to other RD-Connect tools, providing a link to the
RD-Connect Sample Catalogue, which was developed using MOLGENIS, a large inventory of
RD biological samples available in participating biobanks for RD research. 


%\textbf{GAAIN \cite{gaain}}
%The primary objective of the Global Alzheimer's Association Interactive Network (GAAIN)
%is to establish a virtual community for sharing Alzheimer's-related data stored in
%independently-operated repositories around the world.
%Neuroimaging, demographic, genetic, and biologic data are integrated together while
%respecting the boundaries of existing repositories and protecting the ownership of
%shared data. 
%The prominence of a data sharing network depends upon providing search functionality to
%those looking for data while addressing the concerns of those sharing data.
%As such, GAAIN aims to help scientists find Alzheimer's data for their research while
%protecting the data ownership rights of each of its data partners.
%The system architecture of GAAIN contains a central server that com- municates with
%multiple client applications (Data Partner Clients or DPC's) that are installed at the
%data partner sites.
%Data is locally exported into CSV files and loaded into the DPC's.
%The search interfaces in GAAIN are designed to meet the different needs of GAAIN
%investigators.
%The GAAIN Scoreboard helps investigators quickly determine if data exists in sufficient
%numbers to meet study objectives.
%Periodically, the GAAIN central server will check the online status of each DPC and
%will record when it is offline.
%Whenever a DPC is offline, the Scoreboard displays red “X”s instead of subject counts
%for the data partner.
%The GAAIN Interrogator allows investigators to view data trends and relationships
%before being shown where to obtain the actual data.

\subsection*{Cafe Variome}
Is a general purpose data discovery platform, using design principles that stress
compliance with relevant and emerging standards, ease of use for system operators and
discovery users, with extensive options for customized installation to suit many
different areas of need.

This particular ''genotype-to-phenotype`` application is of broad current interest,
though the technology is equally applicable to other areas.
Cafe Variome~\cite{cafevariome} comprises a single, lightweight software package, which can be easily
installed and configured locally.
The system acts to provide a ''shop window`` into extant data, and is explicitly not
intended as a ''database`` for storing, curating, or integrating information.
Alternatively, original and larger databases do not have to be copied into the
database, but can instead be remotely consulted via the platform’s API.
This same approach of API-based remote data interrogation can also facilitate discovery
on large data files that carry details about individual samples or patients being made
discoverable.

The administrator determines which selection of data fields may be employed for
discovery and/or results display, and also which records are made available for
discovery by any specific user.
The total count of records that match the search term(s) are displayed to the user,
split by data source and also split into three results categories as follows:
\begin{itemize}
    \item openAccess: records for which the particular user is allowed to see the data
        held in the system.
    \item linkedAccess: records for which the particular user may not directly see the
        data held in the system, though a source data link or source contact
        information is provided.
    \item restrictedAccess: records for which the particular user may complete and
        submit to the data owner a data access request.
\end{itemize}

\subsection*{Mica}
% TODO cite maelstrom
Mica~\cite{mica} is used to create websites and metadata portals for individual epidemiological
studies or multi-study consortia, with a specific focus on supporting observational
cohort studies.
The Mica application helps data custodians and study or network coordinators to
efficiently organize and disseminate information about their studies and networks
without significant technical effort.
Mica is made up of a number of different modules developed to add and edit descriptive
information pertaining to epidemiological research networks, studies and datasets.
The network description module allows Mica users to disseminate information such as
network description and the number, design and geographical coverage of participating
studies.
A study description module is used to assemble and publish information on
epidemiological studies such as participant selection criteria, sample size and data
collection timelines.
A dataset module supports the display and dynamic search of study data dictionaries.
For a consortium of studies, a special dataset module also allows documenting and
presenting variables targeted for harmonization and harmonization results.
Once populated with study and variable metadata, Mica includes a powerful search engine
which allows investigators to quickly find the information they need for their research
projects.
In a Mica instance with multiple studies, users can quickly identify a list of studies
with a given profile, which collect data on a specific health outcome, risk factors of
interest and confounding factors.
Connecting Mica to one or more Opal database(s), which will be mentioned on the next
section, allows users to search beyond the metadata by securely querying the actual
study data hosted on remote servers.

%\textbf{Opal \cite{mica}:}
%Opal provides a centralized web-based data management system allowing study
%coordinators and data managers to securely import/export a variety of data types and
%formats using a point-and-click interface.
%Opal then converts, stores and displays these data under a standardized model.
%Opal can also read data directly from other data source engines.
%Once data have been imported, the Opal web application facilitates data curation and
%quality control procedures and allows automated descriptive statistics computation with
%graphical displays such as bar charts and scatter plots.
%This then facilitates metadata browsing and data discoverability using the Mica web
%portal, mentioned in the previous section.
%To ensure privacy, Opal stores participant identifiers in a distinct and
%secure database and provides administrators with tools to manage them.
%When used across multiple studies, Opal is a powerful tool to harmonize epidemiological
%study data.


\subsection*{BioSharing}
BioSharing~\cite{biosharing} is a curated, searchable portal of linked information on content standards,
databases, and journal and funder data policies in the life sciences, ensuring that
they are registered, informative and discoverable.
As a one-stop shop for three types of data mentioned in the life sciences, BioSharing not only links these resources, but also details the relationships between them, providing context and metrics, as standards evolve and are implemented in databases, and both standards and databases are recommended by journal or funding body data policies.

BioSharing also provides a historical perspective on these standards and databases,
detailing when different versions of a standard are created or deprecated, and when
updates to a database or policy appear, so enabling users to assess the maturity and
evolution of each resource.

Although all records in BioSharing are manually curated, many have been added and
edited by the community themselves, rather than BioSharing curators.
Users are able to claim the record(s) for the resource(s) they maintain.
This allows them to not only gain personal recognition for their work, but also ensures
that the data on their resource is accurate and completely up-to-date.
This community curation aspect of BioSharing, along with the linking and embedding of
each record into the landscape of standards and databases, helps make BioSharing an
accurate and comprehensive representation of metadata standards, databases and policies
in the life sciences.

Recently the project changed name to FAIRSharing \cite{fairsharing}.

\subsection*{Dataverse}
The Dataverse Project~\cite{dataverse} is an open source web application to share, preserve, cite, explore, and analyze research data.
Before the Dataverse Project, researchers were forced to choose between receiving credit for their data, by controlling distribution themselves but without long term preservation guarantees, or having long term preservation guarantees, by sending it to a professional archive but without receiving much credit.
The Dataverse Project breaks this bad choice: it facilitates making data available to others, and allows to replicate others' work more easily, giving the deserved credit and web visibility to the data creators.

A Dataverse repository is an installation of Dataverse, which hosts multiple virtual archives called Dataverse collections.
Each Dataverse collection contains datasets, and each dataset contains descriptive metadata and data files.


\subsection*{NADA}
\gls{nada}~\cite{nada} is a web-based cataloging application that allows for the creation of portals that allows users to browse, search, compare, apply for access, and download relevant census or survey information.

It was originally developed to support the establishment of national survey data archives.
The application is used by a diverse and growing number of national, regional, and international organizations.

By default, when a \gls{nada} instance is installed, the catalog created is the Central Data Catalog.
All studies uploaded to it are visible, searchable and accessible.
Studies are carried out to gather more knowledge about a subject.
The \gls{nada} uses the Data Documentation Initiative (DDI) standard for the presentation of metadata for each study, where such documents are prepared outside the \gls{nada} application.
The detailed information about the survey is searchable down to the variable level for each survey in the catalog.

The \gls{nada} allows for the level of access to datasets for studies to be controlled at the study level.
In other words the level of restriction can differ from study to study: data not available, direct access data files, public use data files, authorized access to data files, data available in an enclave and data available from external repository.

Citations are references that can be included at the study level which point to published works that have used the data from a particular study.
Such resources are useful to researchers who are interested in seeing how the data have been used before.
They are also a good way of showing the funders of surveys that the data are being used for policy and research purposes and thus are an indicator of some of the impact a study has had.

\section{Metadata Extraction Tools}

Associated with some projects mentioned in the previous section, several tools and
processes are relevant for the task of extracting/profiling datasets.
Next are presented such tools:

\subsection*{ACHILLES}
The \gls{ohdsi} project offers a wide range of open-source tools\cite{ohdsi-tools} to support various data-analytics use cases on observational patient-level data.
What these tools have in common is that they can all interact with one or more databases using the \gls{cdm}.
Furthermore, these tools standardize the analytics for various use cases.

\gls{achilles}~\cite{achilles-github} is a software tool that provides summaries and metadata of a database conforming to the \gls{cdm}.
As such, it can be used by researchers to evaluate which databases conform better to their needs.
However, certain summaries might reveal some information of the original data that the data owner is not willing to share or because is sensitive patient information.
With this, variations of these tools are creates where only certain summaries are extracted from the data source, removing the process of having to filter the output of the \gls{achilles}.
Such a tool was developed associated with the \gls{ehden} project~\cite{peters-tool}.

% TODO its implemented by a series of sql queries over the original data in R

\subsection*{DataMed}
The mission of DataMed~\cite{datamed} is to provide a data discovery index to help users efficiently find and access existing datasets that are distributed across a wide range of repositories in the biomedical domain.
DataMed developed a metadata ingestion pipeline which extracts, maps, and indexes by following the \gls{dats}, that describes the metadata elements and the structure for datasets \cite{dats}, based on input from the community and a thorough analysis of existing metadata from popular repositories.

It uses a horizontally scalable message oriented extract-transform-load system.
The pipeline is a loosely coupled distributed system consisting of a message dispatcher and one or more data processing components.
The dispatcher acts as a hub, orchestrating the data ingestion and processing pipeline using persistent queues.
Each consumer does operations such as a transformation, cleanup, and/or enhancement on the document, saves the updated document to a document database, and then places a message in the message queue for the dispatcher.
The dispatcher uses the pipeline specification to decide the next step and places a message in the corresponding consumer’s input message queue.

To handle heterogeneity in the data availability from multiple sources, the pipeline abstracts retrieval modes and data formats.
Different ingestors for retrieval mode and data format combinations are developed as specific consumers using a specialized plug-in interface.
Each specific ingestor uses data iterator(s), which allow streaming to retrieve data only when needed, facilitating the processing of datasets much larger than the system memory.
The ingested raw data is transformed into the \gls{dats}.

There was already some work on mapping datasets represented in with the \gls{omop} \gls{cdm} in \cite{cdm-dats}

%add this ? 33451426,MARMoSET - Extracting Publication-ready Mass Spectrometry Metadata from RAW Files.

\section{Metadata Network Architectures}  % TODO discuss

Considering that we have an agent that extracts or gathers metadata from a data source,
it is also important to think on how they will transfer or communicate data with the
interface that clinical researchers use.
Next it is presented some projects that design some sort of network architecture to
retrieve data from the data owners site:

% https://en.wikipedia.org/wiki/ISO/IEC_11179#cite_note-1

\subsection*{CafeVariome}
Cafe Variome~\cite{cafevariome} was designed to be used with safe or sensitive datasets.
In the latter case, there is often a need to limit which persons might access the system to undertake data discovery.
This implies laboratories connecting together in closed or semiclosed discovery networks.
These could be all-to-all networks (where each group places a discovery interface over their own data, for use by other network members) or a hub and spokes arrangement (where the network establishes just one discovery portal that federates searches across all the partner sites), or a combination of these architectures.
Regardless of the preferred arrangement, each group might wish to fine-tune which records or fields are made available for discovery by (and potentially then shared with) each other group or individual in the network.

\subsection*{GAAIN}
The primary objective of the \gls{gaain}~\cite{gaain} is to establish a virtual community for sharing Alzheimer's-related data stored in independently-operated repositories around the world.
Neuroimaging, demographic, genetic, and biologic data are integrated together while respecting the boundaries of existing repositories and protecting the ownership of shared data.
The system architecture of \gls{gaain} contains a central server that communicates with multiple client applications that are installed at the data partner sites (\gls{dpc}).
The \gls{gaain} \gls{dpc}s at each data partner site does not interfere with or consume resources of the local production system.
Since it does not have direct access to the production database, since data is locally exported into CSV files and loaded into the \gls{dpc}', it cannot disrupt the normal operations of the production system.
The data stored in the \gls{dpc} may be updated at the convenience of the data partner, and the only institutional requirement is to change local firewall configurations to allow HTTPS traffic from the central server into the data partner's network.
\gls{gaain} data partners retain complete control over their data.
Every \gls{dpc} has an “on/off switch” which provides the freedom to immediately disconnect data from the network at any time for any reason.

When \gls{gaain} investigators query the network through its web interfaces, search requests are sent to the central server.
The database in each \gls{dpc} is queried and the results are sent back to the central server where they are aggregated into the response passed on to the web interfaces.

\subsection*{PopMedNet}
Popmednet~\cite{popmednet} is a software platform designed to facilitate the creation and operation of distributed health data networks and to meet the needs of disparate data partners, coordinating center models, and researchers.
The platform is a flexible architecture and governance models enable network designs that meet the critical needs of data partners within distributed networks, including data privacy and security requirements, system security requirements, governance and operational requirements, regulatory and workflow requirements, and monitoring of network functions.
PopMedNet includes a number of features designed to facilitate research and network learning more broadly than simply distributing queries.
These features aid researchers in identifying potential collaborators, discovering prior research conducted within their network, and understanding more about the data available within their network.

The PopMedNet platform consists of two interrelated components: a web-based portal for distributing requests and administering the network, and the DataMart Client.
Both components are combined together through a publish-and-subscribe approach, that does not require any open ports, eliminating a critical security concern for data partners.
The DataMart Client is installed on a data partner end user’s local machine, behind the data partner firewall.
There is no direct external access to local data and all queries from the network portal are pulled into the local environment rather than being pushed through an open port.
The DataMart Client acts as an inbox for data partners to receive, review, and respond to queries distributed from a network portal.
This enables data partners to review the details of all requests, including request metadata such as the name and email address of the requester, a description of the request, the purpose of the request, and the request parameters.
After review, the data partner may choose to execute the query, hold it for further review, or reject it.
This asynchronous approach to querying is a feature of the system that provides the data partners with complete control over their data and all its uses.
Data partners can choose to automate many of the query processing steps, or choose to use the manual process to ensure compliance with local requirements.

\subsection*{EHR4CR}

The EHR4CR~\cite{ehr4cr} project aims to build a robust and scalable platform that will unlock data from hospital EHR systems, in full compliance with the ethical, regulatory and data protection policies and requirements of each participating country.

%Information representation
The developed solutions allow identifying patient cohorts and extracting patient-centric data using distributed EHRs/Clinical data warehouses (CDWs).

%Platform services
The access to the clinical data locally at the data endpoints consists of three logical layers: the legacy system layer (specific to the type of CDW or EHR used by the local site), the Legacy Interface layer and the EHR4CR Data source Endpoint layer.
The Legacy Interface layer deals with the complexity involved when accessing the various types of CDW and EHR systems used and the adopted strategy for translating queries against the EHR4CR information model and pivot terminology into queries that can be executed locally against the CDW or EHR system.
The EHR4CR Data source Endpoint layer is a generic layer that exposes uniform EHR4CR endpoint interfaces to other EHR4CR services and components.
Once approved, the service provider metadata is added to the central registry and exposed services and applications can be published so that platform users can discover them.

% merge the two following paragraphs into one
The current architecture description focuses on the Protocol Feasibility Scenario (PFS) and Patient identification and Recruitment Scenario (PRS).
The most interesting scenario is PFS where The main components involved are:
• Protocol feasibility tools in the form of a workbench for studying non-identifiable distributed patient data.
• An orchestration module allowing distributed execution of eligibility criteria queries.
• Endpoint (data access) services allowing eligibility criteria query execution on local clinical data warehouse facilities.

The process of querying individual data endpoints for protocol feasibility is initiated by the workbench instance on behalf of an end-user.
The workbench then submits a series of Eligibility Criteria (EC) queries to an orchestrator service instance.
The orchestrator instance identifies and invokes the data endpoints to which the EC queries are targeted.
Finally after receiving the individual EC query results, the orchestrator service instance provides a consolidated result to the workbench instance which will eventually be displayed to the end-user.

security
In order to encompass existing local data provider security policies and firewall rules, the platform supports the invocation of web services using dynamically configurable transport bindings.
Examples include asynchronous web service invocation by employing message-oriented middleware to provide the ability (from an endpoint provider perspective) to retrieve (pull) incoming queries rather than receiving these directly (push), thus avoiding the need for endpoint providers to accept incoming connections from the Internet into their local network.
This feature ensures compliance with local data provider policies, thus facilitating platform adoption, while at the same time allowing for standards-based authentication and authorization of end-users and web service clients operating on their behalf.

\subsection*{NextGen Connect}
% User guide - https://www.nextgen.com/-/media/files/nextgen-connect/nextgen-connect-310-user-guide.pdf
% NextGen Health Care
NextGen Connected Health helps many of the nation’s largest, most respected healthcare entities streamline their
care-management processes to satisfy the demands of a regulatory, competitive healthcare industry.
With NextGen Connect, NextGen Healthcare's goal is to provide the healthcare community with a secure, efficient, cost effective means of sharing health information.

% NextGen Connect
Like an interpreter who translates foreign languages into the one you understand, NextGen Connect Integration Engine translates message standards into the one your system understands.
Whenever a "foreign" system sends you a message, NextGen Connect Integration Engine’s integration capabilities expedite the following:
Filtering – reads message parameters and passes the message to or stops it on its way to the transformation stage.
Transformation – converts the incoming message standard to another standard (e.g., HL7 to XML) (Popular Medical standards supported, ex: DICOM, HL7). Such tranformations cam also be more generic and custom by adding additional javascript or Java code.
Extraction – can "pull" data from and "push" data to a database.
Routing – makes sure messages arrive at their assigned destinations.

The interfaces you configure that perform these jobs are called channels:
A channel consists of multiple connectors.
A connector is a piece of a channel that does the job of getting data into NextGen Connect (a source connector), or sending data out to an external system (a destination connector).
Every channel has exactly one source connector, and at least one destination connector.
For example you may receive data over HTTP, then write the data out to a file somewhere, and also insert pieces of the data into your custom database.

%https://github.com/nextgenhealthcare/connect


\section{Findings}

\begin{table}[H]
    \center
    \begin{tabular}{|*{6}{c |}}
\hline 
        \multirow{2}{*}{Tool Name} & \multirow{2}{*}{Open Source} & \multicolumn{2}{c|}{Visualization/Interaction} & \multirow{2}{*}{Extraction} & \multirow{2}{*}{Network} \\
\cline{3-4}
        & & Data protection  & FAIR & &   \\
\hline
        eGenVar \cite{egenvar} & {\color{green} \cmark} \repo{https://github.com/Sabryr/EGDMS} & {\color{green} \cmark} (Users + Permissions)& {\color{green} \cmark} & {\color{red} \xmark} & {\color{red} \xmark} \\
\hline
        MONTRA \cite{montra} & {\color{green} \cmark} \repo{https://github.com/bioinformatics-ua/montra} & {\color{green} \cmark} (Role based) & {\color{green} \cmark} & {\color{red} \xmark} &  {\color{red} \xmark} \\
\hline
        REDCap \cite{redcap} & {\color{red} \xmark} & {\color{green} \cmark} (Role based) & {\color{green} \cmark} & {\color{red} \xmark} & {\color{red} \xmark}  \\
\hline
        Data Sphere \cite{datasphere} & {\color{red} \xmark} & {\color{green} \cmark} (Authorized Users Only) & {\color{red} \xmark} & {\color{red} \xmark} & {\color{red} \xmark} \\
\hline
        MOLGENIS \cite{molgenis} & {\color{green} \cmark} \repo{https://github.com/molgenis/molgenis} & {\color{green} \cmark} (Role based) & {\color{red} \xmark} & {\color{red} \xmark} & {\color{red} \xmark} \\
\hline
        Cafe Variome \cite{cafevariome} & {\color{red} \xmark} & {\color{green} \cmark} (Role based) & {\color{green} \cmark} & {\color{red} \xmark} & {\color{green} \cmark} \\
\hline
        Mica \cite{mica} & {\color{green} \cmark} \repo{https://github.com/obiba/mica2} & {\color{red} \xmark} & {\color{green} \cmark} & {\color{red} \xmark} & {\color{red} \xmark} \\
\hline
        BioSharing \cite{biosharing} & {\color{green} \cmark} \repo{https://github.com/FAIRsharing/fairsharing.github.io/} & {\color{red} \xmark} & {\color{green} \cmark} & {\color{red} \xmark} & {\color{red} \xmark} \\
\hline
        Dataverse \cite{dataverse} & {\color{green} \cmark} \repo{https://github.com/IQSS/dataverse} & {\color{green} \cmark} (Role Based) & {\color{green} \cmark} & {\color{red} \xmark} & {\color{red} \xmark} \\
\hline
        NADA \cite{nada} & {\color{green} \cmark} \repo{https://github.com/ihsn/nada} & {\color{green} \cmark} (Access Request) & {\color{red} \xmark} & {\color{red} \xmark} & {\color{red} \xmark} \\
\hline
\hline
        ACHILLES \cite{achilles-github} & {\color{green} \cmark} \repo{https://github.com/OHDSI/Achilles/} & \multicolumn{2}{c|}{\color{red} \xmark} & {\color{green} \cmark} & {\color{red} \xmark} \\
\hline
        DataMed \cite{datamed} & {\color{green} \cmark} \repo{https://github.com/biocaddie} & \multicolumn{2}{c|}{\color{red} \xmark} & {\color{green} \cmark} & {\color{red} \xmark} \\
\hline
\hline
        GAAIN \cite{gaain} & {\color{red} \xmark} & \multicolumn{2}{c|}{\color{red} \xmark} & {\color{red} \xmark} & {\color{green} \cmark} \\
\hline
        PopMedNet \cite{popmednet} & {\color{red} \xmark} & \multicolumn{2}{c|}{\color{red} \xmark} & {\color{red} \xmark} & {\color{green} \cmark} \\
\hline
        EHR4CR \cite{ehr4cr} & {\color{red} \xmark} & \multicolumn{2}{c|}{\color{red} \xmark} & {\color{red} \xmark} & {\color{green} \cmark} \\
\hline
        NextGen Connect & {\color{green} \cmark} \repo{https://github.com/nextgenhealthcare/connect} & \multicolumn{2}{c|}{\color{red} \xmark} & {\color{red} \xmark} & {\color{green} \cmark} \\
\hline
\end{tabular}
\end{table}

% Similarity between extraction tools -> all converge different data model to a comon one

